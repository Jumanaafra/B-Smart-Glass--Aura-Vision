// screens/VisuallyImpairedMain/VisuallyImpairedMain.tsx

import React, { useState, useEffect, useRef } from 'react';
import { Page } from '../../types';
import { Icon } from '../../components/Icon';
import { io } from "socket.io-client"; // Socket Client
import './VisuallyImpairedMain.css';

interface VisuallyImpairedMainProps {
  setPage: (page: Page) => void;
}

export const VisuallyImpairedMain: React.FC<VisuallyImpairedMainProps> = ({ setPage }) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  
  // UI States
  const [status, setStatus] = useState("Tap screen to ask...");
  const [isListening, setIsListening] = useState(false);
  const [aiSpeaking, setAiSpeaking] = useState(false);
  const [socket, setSocket] = useState<any>(null);
  const [userQuery, setUserQuery] = useState("");
  const [aiReply, setAiReply] = useState("");
  
  // Language State (Default: English)
  const [language, setLanguage] = useState<'EN' | 'TG'>('EN');

  // 1. DEFAULT BROWSER VOICE (ElevenLabs-‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Æ§‡Æø‡Æ≤‡Øç)
  const speak = (text: string) => {
    window.speechSynthesis.cancel(); // ‡Æ™‡Æ¥‡Øà‡ÆØ ‡Æ™‡Øá‡Æö‡Øç‡Æö‡Øà ‡Æ®‡Æø‡Æ±‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ
    const utterance = new SpeechSynthesisUtterance(text);
    
    // ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Æ©‡Øç ‡Æá‡Æô‡Øç‡Æï‡Æø‡Æ≤‡ØÄ‡Æ∑‡Øç ‡Æµ‡Ææ‡ÆØ‡Øç‡Æ∏‡Øç ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ©‡ØÅ ‡Æ§‡Øá‡Æü‡ØÅ‡Æ±‡Øã‡ÆÆ‡Øç (Tanglish ‡Æ®‡Æ≤‡Øç‡Æ≤‡Ææ ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç)
    const voices = window.speechSynthesis.getVoices();
    const preferredVoice = voices.find(v => v.lang.includes('IN') || v.name.includes('India')) || voices.find(v => v.lang.includes('US'));
    if (preferredVoice) utterance.voice = preferredVoice;

    utterance.rate = 0.9; // ‡Æï‡Øä‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡ÆÆ‡ØÜ‡Æ§‡ØÅ‡Æµ‡Ææ‡Æï
    utterance.pitch = 1.0;
    
    utterance.onstart = () => setAiSpeaking(true);
    utterance.onend = () => setAiSpeaking(false);
    
    window.speechSynthesis.speak(utterance);
  };

  // 2. Socket Connect & Camera Start (Immediately)
  useEffect(() => {
    const newSocket = io("https://b-smart-glass-aura-vision.onrender.com");
    setSocket(newSocket);

    const startCamera = async () => {
      try {
        // ‡ÆÆ‡Øä‡Æ™‡Øà‡Æ≤‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æï ‡Æ™‡Æø‡Æ©‡Øç‡Æ™‡Æï‡Øç‡Æï ‡Æï‡Øá‡ÆÆ‡Æ∞‡Ææ
        let stream;
        try {
            stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
        } catch (e) {
            // ‡Æ≤‡Øá‡Æ™‡Øç‡Æü‡Ææ‡Æ™‡Øç‡Æ™‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ§‡ØÅ ‡Æá‡Æ≤‡Øç‡Æ≤‡Øà‡ÆØ‡ØÜ‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç, ‡Æö‡Ææ‡Æ§‡Ææ‡Æ∞‡Æ£ ‡Æï‡Øá‡ÆÆ‡Æ∞‡Ææ
            stream = await navigator.mediaDevices.getUserMedia({ video: true });
        }

        if (videoRef.current) videoRef.current.srcObject = stream;
        
        // ‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡ØÅ: ‡ÆÜ‡Æ™‡Øç ‡Æì‡Æ™‡Æ©‡Øç ‡ÆÜ‡Æ© ‡Æâ‡Æü‡Æ©‡Øá ‡Æµ‡Ææ‡ÆØ‡Øç‡Æ∏‡Øç ‡Æµ‡Æ∞‡Ææ‡Æ§‡ØÅ (Browser Policy). 
        // ‡Æ®‡ØÄ‡Æô‡Øç‡Æï ‡Æ∏‡Øç‡Æï‡Æø‡Æ∞‡ØÄ‡Æ©‡Øà ‡Æ§‡Øä‡Æü‡Øç‡Æü ‡Æ™‡Æø‡Æ±‡Æï‡ØÅ ‡Æ§‡Ææ‡Æ©‡Øç ‡Æ™‡Øá‡Æö ‡ÆÜ‡Æ∞‡ÆÆ‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.
        speak("I am ready."); 
      } catch (err) {
        console.error("Camera Error:", err);
        setStatus("Camera Error üî¥");
        speak("Camera error.");
      }
    };
    startCamera();

    return () => { newSocket.close(); };
  }, []);

  // 3. Live Stream Loop (Fast Video)
  useEffect(() => {
    if (!socket) return;
    
    const interval = setInterval(() => {
      if (videoRef.current) {
        const canvas = document.createElement('canvas');
        canvas.width = videoRef.current.videoWidth / 2;
        canvas.height = videoRef.current.videoHeight / 2;
        const ctx = canvas.getContext('2d');
        if(ctx) {
            ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
            const imageBase64 = canvas.toDataURL('image/jpeg', 0.5);
            socket.emit('send-video-frame', { image: imageBase64 });
            
             navigator.geolocation.getCurrentPosition((pos) => {
                socket.emit('send-location', { lat: pos.coords.latitude, lng: pos.coords.longitude });
            }, () => {}, {enableHighAccuracy: true});
        }
      }
    }, 200); 

    return () => clearInterval(interval);
  }, [socket]);

  // 4. Language Toggle
  const toggleLanguage = (e: React.MouseEvent) => {
      e.stopPropagation(); 
      const newLang = language === 'EN' ? 'TG' : 'EN';
      setLanguage(newLang);
      speak(newLang === 'TG' ? "Tanglish mode on" : "English mode on");
  };

  // 5. Voice Input
  const startListening = () => {
    if (isListening || aiSpeaking) {
        window.speechSynthesis.cancel();
        setAiSpeaking(false);
        return; 
    }
    
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert("Voice input not supported. Use Chrome.");
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.lang = 'en-US'; 
    recognition.start();
    setIsListening(true);
    setStatus(language === 'TG' ? "Kelunga... (Listening) üé§" : "Listening... üé§");

    recognition.onresult = async (event: any) => {
      const transcript = event.results[0][0].transcript;
      setUserQuery(`"${transcript}"`);
      setIsListening(false);
      await analyzeImage(transcript);
    };

    recognition.onerror = () => {
      setStatus("Try again.");
      speak("I didn't catch that.");
      setIsListening(false);
    };
    
    recognition.onend = () => setIsListening(false);
  };

  // 6. AI Process
  const analyzeImage = async (query: string) => {
    if (!videoRef.current) return;
    const canvas = document.createElement('canvas');
    canvas.width = videoRef.current.videoWidth;
    canvas.height = videoRef.current.videoHeight;
    const ctx = canvas.getContext('2d');
    if (ctx) ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);
    const imageBase64 = canvas.toDataURL('image/jpeg', 0.6);

    setStatus(language === 'TG' ? "Yosikkiren... üß†" : "Thinking... üß†");
    
    try {
      const response = await fetch('https://b-smart-glass-aura-vision.onrender.com/api/ai/describe', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ imageBase64, prompt: query, language }),
      });

      const data = await response.json();
      const reply = data.description || "Server Error.";
      
      setAiReply(reply);
      setStatus(language === 'TG' ? "Badhil vandhuchu ‚úÖ" : "Done ‚úÖ");
      speak(reply); // Browser Voice-‡Æ≤‡Øç ‡Æ™‡Øá‡Æö‡ØÅ‡ÆÆ‡Øç

    } catch (error) {
      setStatus("Connection Error ‚ùå");
      speak("Internet error.");
    }
  };

  return (
    <div className="vim-container" onClick={startListening}>
      <header className="vim-header">
        <h1 className="vim-header-title">IRIS Assistant</h1>
        <div className="vim-header-actions">
            <button onClick={toggleLanguage} className="vim-lang-button">
                {language}
            </button>
            <div onClick={(e) => e.stopPropagation()}>
                <button onClick={() => setPage(Page.SETTINGS)}><Icon name="settings" className="vim-icon-settings"/></button>
            </div>
        </div>
      </header>

      <div className="vision-active-container">
        <video ref={videoRef} autoPlay playsInline muted className="vision-active-bg-img" />
        
        <div className="vim-status-box">
            <p>{status}</p>
        </div>

        {isListening && (
            <div className="vision-active-overlay">
                <p className="vision-active-text">Listening...</p>
            </div>
        )}
        
         {aiSpeaking && !isListening && (
             <div className="vision-active-overlay" style={{backgroundColor: 'rgba(0,0,0,0.3)'}}>
                <p className="vision-active-text">Speaking...</p>
            </div>
        )}
      </div>
      
      <div className="vim-conversation-box">
          <p className="vim-user-text">{userQuery}</p>
          <p className="vim-ai-text">{aiReply}</p>
      </div>

      <footer className="vim-footer">
        <div className="footer-content">
            <button className={`speak-button ${isListening ? 'speak-button-listening' : 'speak-button-default'}`}>
                <Icon name="microphone" className="speak-button-icon" />
            </button>
            <p className="speak-button-label">Tap to Speak</p>
        </div>
      </footer>
    </div>
  );

};
